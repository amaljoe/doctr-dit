{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae6f343e-cab5-489f-8fab-918467d9e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 1,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compiling-ganesh/24m0797/workspace/doctr-dit/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:1621: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2852, device='cuda:0', grad_fn=<NllLossBackward0>) torch.Size([1, 312, 256]) tensor([[  2,   0,   0,   0, 220, 220, 220,  79,  79,  79, 134, 134, 134,  41,\n",
      "          41,  41, 222, 222, 222, 253, 253, 253, 119, 119, 119,  81,  81,  81,\n",
      "           5,   5,   5, 236, 236, 236, 207, 207, 207, 137, 137, 137, 161, 161,\n",
      "         161, 232, 232, 232, 215, 215, 215,  14,  14,  14, 147, 147, 147, 226,\n",
      "         226, 226,   4,   4,   4,  75,  75,  75, 247, 247, 247, 219, 219, 219,\n",
      "          46,  46,  46,  24,  24,  24,   9,   9,   9, 146, 146, 146,  86,  86,\n",
      "          86, 165, 165, 165,  99,  99,  99, 239, 239, 239, 195, 195, 195, 248,\n",
      "         248, 248, 120, 120, 120,  12,  12,  12,  71,  71,  71, 169, 169, 169,\n",
      "         161, 161, 201, 201, 201, 212, 212, 212,  46,  46, 113, 113, 113,  46,\n",
      "          46,   2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoProcessor, BartForConditionalGeneration, BartConfig\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from datasets import load_dataset\n",
    "\n",
    "class LayoutLMv3BART(nn.Module):\n",
    "    def __init__(self, encoder_name=\"microsoft/layoutlmv3-base\", num_labels=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # ----- Encoder -----\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        hidden_size = self.encoder.config.hidden_size  # 768\n",
    "\n",
    "        # ----- BART Decoder -----\n",
    "        config = BartConfig.from_pretrained(\"facebook/bart-base\")\n",
    "        config.encoder_layers = 0                     # remove BART encoder\n",
    "        config.d_model = hidden_size                  # match LayoutLMv3 dim\n",
    "        config.vocab_size = num_labels                # segment/NER label count\n",
    "        config.max_position_embeddings = 1024         # safe upper bound\n",
    "        config.num_hidden_layers = 1\n",
    "        print(config)\n",
    "        self.decoder = BartForConditionalGeneration(config)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                bbox,\n",
    "                pixel_values,\n",
    "                attention_mask,\n",
    "                labels=None):\n",
    "\n",
    "        # ----- Encode -----\n",
    "        enc_out = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            bbox=bbox,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask\n",
    "        ).last_hidden_state   # shape: [B, 509, 768]\n",
    "\n",
    "        # BART expects encoder_outputs as a tuple\n",
    "        encoder_outputs = (enc_out,)\n",
    "\n",
    "        # ----- Decode -----\n",
    "        out = self.decoder(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            labels=labels,                # BART handles shifting internally\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, bbox, pixel_values, attention_mask, max_length=128):\n",
    "        # Encode like LayoutLMv3\n",
    "        enc_out = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            bbox=bbox,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "    \n",
    "        # WRAP in correct HF object\n",
    "        encoder_outputs = BaseModelOutput(last_hidden_state=enc_out)\n",
    "    \n",
    "        # Now decoder.generate works\n",
    "        pred = self.decoder.generate(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "\n",
    "train_ds = load_dataset(\"parquet\", data_files=\"../data/funsd-v3/funsd/train-00000-of-00001.parquet\", split=\"train\")\n",
    "example = train_ds[5]\n",
    "image = example[\"image\"]\n",
    "words = example[\"tokens\"]\n",
    "boxes = example[\"bboxes\"]\n",
    "labels = example[\"ner_tags\"]\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "encoding = processor(\n",
    "    image,\n",
    "    words,\n",
    "    boxes=boxes,\n",
    "    word_labels=labels,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "word_labels = encoding.pop(\"labels\")\n",
    "\n",
    "model = LayoutLMv3BART(num_labels=256)\n",
    "model = model.to(device)\n",
    "out = model(\n",
    "    input_ids=encoding[\"input_ids\"].to(device),\n",
    "    bbox=encoding[\"bbox\"].to(device),\n",
    "    pixel_values=encoding[\"pixel_values\"].to(device),\n",
    "    attention_mask=encoding[\"attention_mask\"].to(device),\n",
    "    labels=word_labels.to(device)\n",
    ")\n",
    "\n",
    "generated = model.generate(\n",
    "    input_ids=encoding[\"input_ids\"].to(device),\n",
    "    bbox=encoding[\"bbox\"].to(device),\n",
    "    pixel_values=encoding[\"pixel_values\"].to(device),\n",
    "    attention_mask=encoding[\"attention_mask\"].to(device),\n",
    ")\n",
    "\n",
    "print(out.loss, out.logits.shape, generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8312a6b-ec70-4b69-a9b9-ac8bf9cb8666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6faa5a675e5641c5ae474b52e7e1abd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([512]),\n",
       " torch.Size([512]),\n",
       " tensor([   3,    6,    6,    6,    8,    6,    6,    3,    4,    5,    5,    5,\n",
       "            5,    5,    6,    7,    7,    7,    7,    8,    9,    9,    9,    9,\n",
       "            9,    8,    9,    9,    6,    7,    8,    9,    9,    6,    7,    7,\n",
       "            8,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
       "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
       "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
       "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
       "            9,    9,    9,    9,    9,    9,    6,    7,    7,    7,    8,    9,\n",
       "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
       "            9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,\n",
       "            9,    9,    9,    4,    5,    5,    5,    5,    5,    6,    7,    8,\n",
       "            9,    9,    9,    9,    9,    8,    9,    9,    9,    9,    3,    3,\n",
       "            3,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    max_len = 512\n",
    "    images = [img.convert(\"RGB\") for img in batch[\"image\"]]\n",
    "    words = batch[\"tokens\"]\n",
    "    boxes = batch[\"bboxes\"]\n",
    "    word_labels = [[t + 3 for t in tags][:max_len - 1] +  [2] + [-100] * max(max_len - len(tags) - 1, 0) for tags in batch[\"ner_tags\"]]\n",
    "    enc = processor(\n",
    "        images,\n",
    "        words,\n",
    "        boxes=boxes,\n",
    "        word_labels=word_labels,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_len\n",
    "    )\n",
    "    enc['decoder_labels'] = word_labels\n",
    "\n",
    "    return enc\n",
    "\n",
    "train_ds = load_dataset(\"parquet\", data_files=\"../data/funsd-v3/funsd/train-00000-of-00001.parquet\", split=\"train\")\n",
    "train_ds = train_ds.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=train_ds.column_names\n",
    ")\n",
    "train_ds.set_format('pytorch')\n",
    "\n",
    "test_ds = load_dataset(\"parquet\", data_files=\"../data/funsd-v3/funsd/test-00000-of-00001.parquet\", split=\"train\")\n",
    "test_ds = test_ds.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=test_ds.column_names\n",
    ")\n",
    "test_ds.set_format('pytorch')\n",
    "\n",
    "train_ds[0]['input_ids'].shape, train_ds[0]['decoder_labels'].shape, train_ds[0]['decoder_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecbbfeee-3c47-4a92-8eb4-f27f2df94b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 1,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 5/5 [00:09<00:00,  1.83s/it, loss=3.31]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.873741626739502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.72]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1236109733581543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.37]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9227676391601562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.31]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8137649893760681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.17]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7981394231319427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.11]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7640697956085205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.13]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7617313265800476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.18]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.745508998632431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.13]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7329736351966858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.1] \n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7095493674278259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.06]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7204688489437103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.1]  \n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6976237893104553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.08]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7044256031513214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=0.963]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7024900615215302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.05]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6880208551883698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=0.973]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6976084411144257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.03] \n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6843081414699554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.08] \n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6929122507572174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=0.968]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6839370131492615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=0.999]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6900151669979095\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = LayoutLMv3BART(num_labels=256).to(device)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 20\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in loop:\n",
    "\n",
    "        # Move all encoder inputs\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        bbox = batch[\"bbox\"].to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Decoder labels (word-level labels)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            bbox=bbox,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Validation\"):\n",
    "    \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            bbox = batch[\"bbox\"].to(device)\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"decoder_labels\"].to(device)\n",
    "    \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                bbox=bbox,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "    \n",
    "            total_val_loss += outputs.loss.item()\n",
    "    \n",
    "    print(\"Validation Loss:\", total_val_loss / len(test_dataloader))\n",
    "\n",
    "    # acc = exact_match_accuracy(model, test_dataloader)\n",
    "    # print(\"Accuracy:\", acc)\n",
    "\n",
    "torch.save(model.state_dict(), \"../models/layoutlmv3_bart_segmentation.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "795837f7-2414-4c2c-8b28-41a6b41fe2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:42<00:00, 21.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17757222793194016"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def exact_match_accuracy(model, dataloader, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            bbox = batch[\"bbox\"].to(device)\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"decoder_labels\"].to(device)     # [B, T_gt]\n",
    "\n",
    "            B, T_gt = labels.shape\n",
    "\n",
    "            # 1. Generate predictions (can be shorter or longer)\n",
    "            preds = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                bbox=bbox,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=T_gt,\n",
    "            )   # [B, T_pred <= T_gt]\n",
    "\n",
    "            # 2. Pad predictions to match GT length\n",
    "            T_pred = preds.size(1)\n",
    "            if T_pred < T_gt:\n",
    "                pad = torch.full(\n",
    "                    (B, T_gt - T_pred),\n",
    "                    fill_value=-999,   # invalid token (never matches)\n",
    "                    device=device,\n",
    "                    dtype=preds.dtype\n",
    "                )\n",
    "                preds = torch.cat([preds, pad], dim=1)\n",
    "            elif T_pred > T_gt:\n",
    "                preds = preds[:, :T_gt]\n",
    "\n",
    "            # 3. Mask ignore_index in labels\n",
    "            mask = labels != -100\n",
    "\n",
    "            # 4. Count matches\n",
    "            correct = ((preds == labels) & mask).sum().item()\n",
    "            total = mask.sum().item()\n",
    "\n",
    "            total_correct += correct\n",
    "            total_count += total\n",
    "\n",
    "    return total_correct / total_count if total_count > 0 else 0.0\n",
    "\n",
    "exact_match_accuracy(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aae85380-7ba5-48e3-8b3b-39042243c504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223,\n",
       " 223,\n",
       " 512,\n",
       " Dataset({\n",
       "     features: ['id', 'tokens', 'bboxes', 'ner_tags', 'image'],\n",
       "     num_rows: 50\n",
       " }))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds1 = load_dataset(\"parquet\", data_files=\"../data/funsd-v3/funsd/test-00000-of-00001.parquet\", split=\"train\")\n",
    "test_ds2 = test_ds1.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=test_ds1.column_names\n",
    ")\n",
    "test_ds2.set_format('pytorch')\n",
    "len(test_ds1[0]['tokens']), len(test_ds1[0]['ner_tags']), len(test_ds2[0]['input_ids']), test_ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2e7bf8f2-2535-44e8-ab7b-72aad2a74b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TO:',\n",
       " 'DATE:',\n",
       " '3',\n",
       " 'Fax:',\n",
       " 'NOTE:',\n",
       " '82092117',\n",
       " '614',\n",
       " '-466',\n",
       " '-5087',\n",
       " 'Dec',\n",
       " '10',\n",
       " \"'98\",\n",
       " '17',\n",
       " ':46',\n",
       " 'P.',\n",
       " '01',\n",
       " 'ATT.',\n",
       " 'GEN.',\n",
       " 'ADMIN.',\n",
       " 'OFFICE',\n",
       " 'Attorney',\n",
       " 'General',\n",
       " 'Betty',\n",
       " 'D.',\n",
       " 'Montgomery',\n",
       " 'CONFIDENTIAL',\n",
       " 'FACSIMILE',\n",
       " 'TRANSMISSION',\n",
       " 'COVER',\n",
       " 'SHEET',\n",
       " '(614)',\n",
       " '466-',\n",
       " '5087',\n",
       " 'FAX',\n",
       " 'NO.',\n",
       " 'George',\n",
       " 'Baroody',\n",
       " '(336)',\n",
       " '335-',\n",
       " '7392',\n",
       " 'FAX',\n",
       " 'NUMBER:',\n",
       " 'PHONE',\n",
       " 'NUMBER:',\n",
       " '(336)',\n",
       " '335-',\n",
       " '7363',\n",
       " 'NUMBER',\n",
       " 'OF',\n",
       " 'PAGES',\n",
       " 'INCLUDING',\n",
       " 'COVER',\n",
       " 'SHEET:',\n",
       " 'June',\n",
       " 'Flynn',\n",
       " 'for',\n",
       " 'Eric',\n",
       " 'Brown/',\n",
       " '(614)',\n",
       " '466-',\n",
       " '8980',\n",
       " 'SENDER',\n",
       " '/PHONE',\n",
       " 'NUMBER:',\n",
       " 'SPECIAL',\n",
       " 'INSTRUCTIONS:',\n",
       " 'IF',\n",
       " 'YOU',\n",
       " 'DO',\n",
       " 'NOT',\n",
       " 'RECEIVE',\n",
       " 'ANY',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'PAGES',\n",
       " 'PROPERLY,',\n",
       " 'PLEASE',\n",
       " 'CONTACT',\n",
       " 'SENDER',\n",
       " 'AS',\n",
       " 'SOON',\n",
       " 'AS',\n",
       " 'POSSIBLE',\n",
       " 'THIS',\n",
       " 'MESSAGE',\n",
       " 'IS',\n",
       " 'INTENDED',\n",
       " 'ONLY',\n",
       " 'FOR',\n",
       " 'THE',\n",
       " 'USE',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'INDIVIDUAL',\n",
       " 'OR',\n",
       " 'ENTITY',\n",
       " 'TO',\n",
       " 'WHOM',\n",
       " 'IT',\n",
       " 'IS',\n",
       " 'ADDRESSED',\n",
       " 'AND',\n",
       " 'MAY',\n",
       " 'CONTAIN',\n",
       " 'INFORMATION',\n",
       " 'THAT',\n",
       " 'IS',\n",
       " 'PRIVILEGED.',\n",
       " 'CONFIDENTIAL,',\n",
       " 'AND',\n",
       " 'EXEMPT',\n",
       " 'FROM',\n",
       " 'DISCLOSURE',\n",
       " 'UNDER',\n",
       " 'APPLICABLE',\n",
       " 'LAW.',\n",
       " 'If',\n",
       " 'the',\n",
       " 'reader',\n",
       " 'of',\n",
       " 'this',\n",
       " 'message',\n",
       " 'is',\n",
       " 'not',\n",
       " 'the',\n",
       " 'intended',\n",
       " 'recipient',\n",
       " 'of',\n",
       " 'the',\n",
       " 'employee',\n",
       " 'or',\n",
       " 'agent',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'delivering',\n",
       " 'the',\n",
       " 'message',\n",
       " 'to',\n",
       " 'the',\n",
       " 'intended',\n",
       " 'recipient,',\n",
       " 'you',\n",
       " 'are',\n",
       " 'hereby',\n",
       " 'notified',\n",
       " 'that',\n",
       " 'any',\n",
       " 'dissemination,',\n",
       " 'distribution,',\n",
       " 'copying,',\n",
       " 'or',\n",
       " 'conveying',\n",
       " 'of',\n",
       " 'this',\n",
       " 'communication',\n",
       " 'in',\n",
       " 'any',\n",
       " 'manner',\n",
       " 'is',\n",
       " 'strictly',\n",
       " 'prohibited.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'have',\n",
       " 'received',\n",
       " 'this',\n",
       " 'comunication',\n",
       " 'in',\n",
       " 'error,',\n",
       " 'please',\n",
       " 'notify',\n",
       " 'us',\n",
       " 'immediately',\n",
       " 'by',\n",
       " 'telephone',\n",
       " 'and',\n",
       " 'return',\n",
       " 'the',\n",
       " 'original',\n",
       " 'message',\n",
       " 'to',\n",
       " 'us',\n",
       " 'at',\n",
       " 'the',\n",
       " 'address',\n",
       " 'below',\n",
       " 'via',\n",
       " 'the',\n",
       " 'U.',\n",
       " 'S.',\n",
       " 'Postal',\n",
       " 'Service.',\n",
       " 'Thank',\n",
       " 'you',\n",
       " 'for',\n",
       " 'your',\n",
       " 'cooperation.',\n",
       " 'State',\n",
       " 'Office',\n",
       " 'Tower',\n",
       " '/',\n",
       " '30',\n",
       " 'East',\n",
       " 'Broad',\n",
       " 'Street/',\n",
       " 'Columbus,',\n",
       " 'Ohio',\n",
       " '43215',\n",
       " '-3428',\n",
       " 'www',\n",
       " 'ag',\n",
       " 'state',\n",
       " 'oh',\n",
       " 'us',\n",
       " 'An',\n",
       " 'Equal',\n",
       " 'Opportunity',\n",
       " 'Employer',\n",
       " 'Primed',\n",
       " 'an',\n",
       " '12',\n",
       " '/10',\n",
       " '/98']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds1[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bcd93-e569-41d9-b1bc-a377c0ec4e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
