{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ae6f343e-cab5-489f-8fab-918467d9e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2976, device='cuda:0', grad_fn=<NllLossBackward0>) torch.Size([1, 312, 256]) tensor([[  2,   0,  95,  95,  95,  77,  77,  77,  40,  40,  40,  14,  14,  14,\n",
      "         106, 106, 106, 172, 172, 172, 166, 166, 166,  49,  49,  49,  23,  23,\n",
      "          23, 101, 101, 101, 247, 247, 247,  78,  78,  78, 213, 213, 213,  64,\n",
      "          64,  64,  69,  69,  69,  38,  38,  38,  82,  82,  82, 190, 190, 190,\n",
      "          76,  76,  76,  31,  31,  94,  94,  94,  66,  66,  66,  69,  69, 192,\n",
      "         192, 192, 184, 184, 184,  41,  41,  41, 120, 120, 189, 189, 189, 201,\n",
      "         201, 201, 120, 120, 120, 165, 165, 165,  55,  55,  55, 130, 130, 130,\n",
      "         161, 161, 161,  83,  83,  83, 255, 255, 255,  97,  97,  97, 208, 208,\n",
      "         208, 179, 179, 179,  93,  93,  93, 246, 246, 246, 153, 153, 153,  41,\n",
      "          41,   2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BartForConditionalGeneration, BartConfig\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "class LayoutLMv3BART(nn.Module):\n",
    "    def __init__(self, encoder_name=\"microsoft/layoutlmv3-base\", num_labels=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # ----- Encoder -----\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        hidden_size = self.encoder.config.hidden_size  # 768\n",
    "\n",
    "        # ----- BART Decoder -----\n",
    "        config = BartConfig.from_pretrained(\"facebook/bart-base\")\n",
    "        config.encoder_layers = 0                     # remove BART encoder\n",
    "        config.d_model = hidden_size                  # match LayoutLMv3 dim\n",
    "        config.vocab_size = num_labels                # segment/NER label count\n",
    "        config.max_position_embeddings = 1024         # safe upper bound\n",
    "\n",
    "        self.decoder = BartForConditionalGeneration(config)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                bbox,\n",
    "                pixel_values,\n",
    "                attention_mask,\n",
    "                labels=None):\n",
    "\n",
    "        # ----- Encode -----\n",
    "        enc_out = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            bbox=bbox,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask\n",
    "        ).last_hidden_state   # shape: [B, 509, 768]\n",
    "\n",
    "        # BART expects encoder_outputs as a tuple\n",
    "        encoder_outputs = (enc_out,)\n",
    "\n",
    "        # ----- Decode -----\n",
    "        out = self.decoder(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            labels=labels,                # BART handles shifting internally\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, bbox, pixel_values, attention_mask, max_length=128):\n",
    "        # Encode like LayoutLMv3\n",
    "        enc_out = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            bbox=bbox,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "    \n",
    "        # WRAP in correct HF object\n",
    "        encoder_outputs = BaseModelOutput(last_hidden_state=enc_out)\n",
    "    \n",
    "        # Now decoder.generate works\n",
    "        pred = self.decoder.generate(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "\n",
    "train_ds = load_dataset(\"parquet\", data_files=\"../data/funsd-v3/funsd/train-00000-of-00001.parquet\", split=\"train\")\n",
    "example = train_ds[5]\n",
    "image = example[\"image\"]\n",
    "words = example[\"tokens\"]\n",
    "boxes = example[\"bboxes\"]\n",
    "labels = example[\"ner_tags\"]\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "encoding = processor(\n",
    "    image,\n",
    "    words,\n",
    "    boxes=boxes,\n",
    "    word_labels=labels,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "word_labels = encoding.pop(\"labels\")\n",
    "\n",
    "model = LayoutLMv3BART(num_labels=256)\n",
    "model = model.to(device)\n",
    "out = model(\n",
    "    input_ids=encoding[\"input_ids\"].to(device),\n",
    "    bbox=encoding[\"bbox\"].to(device),\n",
    "    pixel_values=encoding[\"pixel_values\"].to(device),\n",
    "    attention_mask=encoding[\"attention_mask\"].to(device),\n",
    "    labels=word_labels.to(device)\n",
    ")\n",
    "\n",
    "generated = model.generate(\n",
    "    input_ids=encoding[\"input_ids\"].to(device),\n",
    "    bbox=encoding[\"bbox\"].to(device),\n",
    "    pixel_values=encoding[\"pixel_values\"].to(device),\n",
    "    attention_mask=encoding[\"attention_mask\"].to(device),\n",
    ")\n",
    "\n",
    "print(out.loss, out.logits.shape, generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d8312a6b-ec70-4b69-a9b9-ac8bf9cb8666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573e6776a3804f51b908794970626020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    images = [img.convert(\"RGB\") for img in batch[\"image\"]]\n",
    "    words = batch[\"tokens\"]\n",
    "    boxes = batch[\"bboxes\"]\n",
    "    word_labels = batch[\"ner_tags\"]\n",
    "\n",
    "    enc = processor(\n",
    "        images,\n",
    "        words,\n",
    "        boxes=boxes,\n",
    "        word_labels=word_labels,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return enc\n",
    "\n",
    "train_ds = load_dataset(\"parquet\", data_files=\"../data/funsd-v3/funsd/train-00000-of-00001.parquet\", split=\"train\")\n",
    "train_ds = train_ds.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=train_ds.column_names\n",
    ")\n",
    "train_ds.set_format('pytorch')\n",
    "\n",
    "test_ds = load_dataset(\"parquet\", data_files=\"../data/funsd-v3/funsd/test-00000-of-00001.parquet\", split=\"train\")\n",
    "test_ds = test_ds.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=test_ds.column_names\n",
    ")\n",
    "test_ds.set_format('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ecbbfeee-3c47-4a92-8eb4-f27f2df94b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 5/5 [00:09<00:00,  1.83s/it, loss=1.94]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.4637821912765503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 5/5 [00:09<00:00,  1.83s/it, loss=1.58]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.3286529779434204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.48]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1660884022712708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 5/5 [00:09<00:00,  1.83s/it, loss=1.35]\n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1454375982284546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it, loss=1.4] \n",
      "Validation: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1301340460777283\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = LayoutLMv3BART(num_labels=256).to(device)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(processed, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in loop:\n",
    "\n",
    "        # Move all encoder inputs\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        bbox = batch[\"bbox\"].to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Decoder labels (word-level labels)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            bbox=bbox,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Validation\"):\n",
    "    \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            bbox = batch[\"bbox\"].to(device)\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                bbox=bbox,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "    \n",
    "            total_val_loss += outputs.loss.item()\n",
    "    \n",
    "    print(\"Validation Loss:\", total_val_loss / len(test_dataloader))\n",
    "\n",
    "torch.save(model.state_dict(), \"../models/layoutlmv3_bart_segmentation.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "795837f7-2414-4c2c-8b28-41a6b41fe2e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model.eval()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      6\u001b[39m     pred = model.generate(\n\u001b[32m      7\u001b[39m         input_ids=item[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].to(device),\n\u001b[32m      8\u001b[39m         bbox=item[\u001b[33m'\u001b[39m\u001b[33mbbox\u001b[39m\u001b[33m'\u001b[39m].to(device),\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m         max_length=\u001b[32m128\u001b[39m\n\u001b[32m     12\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: 'dict' object is not an iterator"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "item = next(next(iter(test_dataloader)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model.generate(\n",
    "        input_ids=item['input_ids'].to(device),\n",
    "        bbox=item['bbox'].to(device),\n",
    "        pixel_values=item['pixel_values'].to(device),\n",
    "        attention_mask=item['attention_mask'].to(device),\n",
    "        max_length=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aae85380-7ba5-48e3-8b3b-39042243c504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223,\n",
       " 223,\n",
       " 512,\n",
       " Dataset({\n",
       "     features: ['id', 'tokens', 'bboxes', 'ner_tags', 'image'],\n",
       "     num_rows: 50\n",
       " }))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds1 = load_dataset(\"parquet\", data_files=\"../data/funsd-v3/funsd/test-00000-of-00001.parquet\", split=\"train\")\n",
    "test_ds2 = test_ds1.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=test_ds1.column_names\n",
    ")\n",
    "test_ds2.set_format('pytorch')\n",
    "len(test_ds1[0]['tokens']), len(test_ds1[0]['ner_tags']), len(test_ds2[0]['input_ids']), test_ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2e7bf8f2-2535-44e8-ab7b-72aad2a74b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TO:',\n",
       " 'DATE:',\n",
       " '3',\n",
       " 'Fax:',\n",
       " 'NOTE:',\n",
       " '82092117',\n",
       " '614',\n",
       " '-466',\n",
       " '-5087',\n",
       " 'Dec',\n",
       " '10',\n",
       " \"'98\",\n",
       " '17',\n",
       " ':46',\n",
       " 'P.',\n",
       " '01',\n",
       " 'ATT.',\n",
       " 'GEN.',\n",
       " 'ADMIN.',\n",
       " 'OFFICE',\n",
       " 'Attorney',\n",
       " 'General',\n",
       " 'Betty',\n",
       " 'D.',\n",
       " 'Montgomery',\n",
       " 'CONFIDENTIAL',\n",
       " 'FACSIMILE',\n",
       " 'TRANSMISSION',\n",
       " 'COVER',\n",
       " 'SHEET',\n",
       " '(614)',\n",
       " '466-',\n",
       " '5087',\n",
       " 'FAX',\n",
       " 'NO.',\n",
       " 'George',\n",
       " 'Baroody',\n",
       " '(336)',\n",
       " '335-',\n",
       " '7392',\n",
       " 'FAX',\n",
       " 'NUMBER:',\n",
       " 'PHONE',\n",
       " 'NUMBER:',\n",
       " '(336)',\n",
       " '335-',\n",
       " '7363',\n",
       " 'NUMBER',\n",
       " 'OF',\n",
       " 'PAGES',\n",
       " 'INCLUDING',\n",
       " 'COVER',\n",
       " 'SHEET:',\n",
       " 'June',\n",
       " 'Flynn',\n",
       " 'for',\n",
       " 'Eric',\n",
       " 'Brown/',\n",
       " '(614)',\n",
       " '466-',\n",
       " '8980',\n",
       " 'SENDER',\n",
       " '/PHONE',\n",
       " 'NUMBER:',\n",
       " 'SPECIAL',\n",
       " 'INSTRUCTIONS:',\n",
       " 'IF',\n",
       " 'YOU',\n",
       " 'DO',\n",
       " 'NOT',\n",
       " 'RECEIVE',\n",
       " 'ANY',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'PAGES',\n",
       " 'PROPERLY,',\n",
       " 'PLEASE',\n",
       " 'CONTACT',\n",
       " 'SENDER',\n",
       " 'AS',\n",
       " 'SOON',\n",
       " 'AS',\n",
       " 'POSSIBLE',\n",
       " 'THIS',\n",
       " 'MESSAGE',\n",
       " 'IS',\n",
       " 'INTENDED',\n",
       " 'ONLY',\n",
       " 'FOR',\n",
       " 'THE',\n",
       " 'USE',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'INDIVIDUAL',\n",
       " 'OR',\n",
       " 'ENTITY',\n",
       " 'TO',\n",
       " 'WHOM',\n",
       " 'IT',\n",
       " 'IS',\n",
       " 'ADDRESSED',\n",
       " 'AND',\n",
       " 'MAY',\n",
       " 'CONTAIN',\n",
       " 'INFORMATION',\n",
       " 'THAT',\n",
       " 'IS',\n",
       " 'PRIVILEGED.',\n",
       " 'CONFIDENTIAL,',\n",
       " 'AND',\n",
       " 'EXEMPT',\n",
       " 'FROM',\n",
       " 'DISCLOSURE',\n",
       " 'UNDER',\n",
       " 'APPLICABLE',\n",
       " 'LAW.',\n",
       " 'If',\n",
       " 'the',\n",
       " 'reader',\n",
       " 'of',\n",
       " 'this',\n",
       " 'message',\n",
       " 'is',\n",
       " 'not',\n",
       " 'the',\n",
       " 'intended',\n",
       " 'recipient',\n",
       " 'of',\n",
       " 'the',\n",
       " 'employee',\n",
       " 'or',\n",
       " 'agent',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'delivering',\n",
       " 'the',\n",
       " 'message',\n",
       " 'to',\n",
       " 'the',\n",
       " 'intended',\n",
       " 'recipient,',\n",
       " 'you',\n",
       " 'are',\n",
       " 'hereby',\n",
       " 'notified',\n",
       " 'that',\n",
       " 'any',\n",
       " 'dissemination,',\n",
       " 'distribution,',\n",
       " 'copying,',\n",
       " 'or',\n",
       " 'conveying',\n",
       " 'of',\n",
       " 'this',\n",
       " 'communication',\n",
       " 'in',\n",
       " 'any',\n",
       " 'manner',\n",
       " 'is',\n",
       " 'strictly',\n",
       " 'prohibited.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'have',\n",
       " 'received',\n",
       " 'this',\n",
       " 'comunication',\n",
       " 'in',\n",
       " 'error,',\n",
       " 'please',\n",
       " 'notify',\n",
       " 'us',\n",
       " 'immediately',\n",
       " 'by',\n",
       " 'telephone',\n",
       " 'and',\n",
       " 'return',\n",
       " 'the',\n",
       " 'original',\n",
       " 'message',\n",
       " 'to',\n",
       " 'us',\n",
       " 'at',\n",
       " 'the',\n",
       " 'address',\n",
       " 'below',\n",
       " 'via',\n",
       " 'the',\n",
       " 'U.',\n",
       " 'S.',\n",
       " 'Postal',\n",
       " 'Service.',\n",
       " 'Thank',\n",
       " 'you',\n",
       " 'for',\n",
       " 'your',\n",
       " 'cooperation.',\n",
       " 'State',\n",
       " 'Office',\n",
       " 'Tower',\n",
       " '/',\n",
       " '30',\n",
       " 'East',\n",
       " 'Broad',\n",
       " 'Street/',\n",
       " 'Columbus,',\n",
       " 'Ohio',\n",
       " '43215',\n",
       " '-3428',\n",
       " 'www',\n",
       " 'ag',\n",
       " 'state',\n",
       " 'oh',\n",
       " 'us',\n",
       " 'An',\n",
       " 'Equal',\n",
       " 'Opportunity',\n",
       " 'Employer',\n",
       " 'Primed',\n",
       " 'an',\n",
       " '12',\n",
       " '/10',\n",
       " '/98']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds1[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bcd93-e569-41d9-b1bc-a377c0ec4e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
